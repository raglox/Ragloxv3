#!/usr/bin/env python3
"""
Test LLM Service for RAGLOX v3.0 - Fixed version
Tests the Blackbox AI provider functionality
"""

import asyncio
import logging
import sys
import os

# Add the src directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from core.config import get_settings
from core.llm import LLMService, BlackboxAIProvider, LLMConfig
from core.llm.base import ProviderType, LLMMessage
from core.llm.service import get_llm_service
from core.llm.prompts import REFLEXION_SYSTEM_PROMPT, extract_json_from_response
from core.llm.models import AnalysisRequest, FailureAnalysis
from core.llm.prompts import build_analysis_prompt

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_llm_service():
    """Test LLM service with Blackbox AI provider"""
    
    print("üß™ Testing LLM Service (Blackbox AI)...")
    
    # Get settings
    settings = get_settings()
    
    print(f"üìÑ LLM Provider: {settings.llm_provider}")
    print(f"üéØ LLM Model: {settings.llm_model}")
    print(f"üîë API Key configured: {'Yes' if settings.effective_llm_api_key else 'No'}")
    
    # Test direct provider initialization
    try:
        config = LLMConfig(
            provider_type=ProviderType.OPENAI,
            api_key=settings.effective_llm_api_key,
            api_base=settings.llm_api_base or "https://api.blackbox.ai",
            model=settings.llm_model or "blackboxai/openai/gpt-4o-mini",
            temperature=settings.llm_temperature,
            max_tokens=settings.llm_max_tokens,
            timeout=30.0,  # Reduce timeout for testing
        )
        
        print("\nü§ñ Initializing Blackbox AI provider...")
        provider = BlackboxAIProvider(config)
        
        # Test provider health check (with fix for context length error)
        print("\nüè• Testing provider health check...")
        try:
            health_status = await provider.health_check()
            print(f"‚úÖ Provider health: {'Healthy' if health_status else 'Unhealthy'}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Health check failed: {e}")
            print("üìù Trying alternative health check...")
            # Try a simpler health check
            try:
                messages = [LLMMessage.user("OK")]
                response = await provider.generate(messages, max_tokens=5, model="gpt-3.5-turbo")
                print(f"‚úÖ Alternative health check passed: {response.content.strip()}")
                health_status = True
            except Exception as e2:
                print(f"‚ùå Alternative health check also failed: {e2}")
                health_status = False
        
        if health_status:
            # Test simple message
            print("\nüí¨ Testing simple message generation...")
            messages = [LLMMessage.user("Hello, this is a test. Please respond with 'Test successful!'")]
            
            response = await provider.generate(messages, max_tokens=20, model="blackboxai/deepseek/deepseek-chat:free")
            print(f"üì§ Response received: {response.content.strip()}")
            print(f"‚è±Ô∏è  Latency: {response.latency_ms:.2f}ms")
            print(f"üìä Tokens used: {response.usage.total_tokens}")
            print(f"üèÅ Finish reason: {response.finish_reason}")
            
            # Test analysis functionality
            print("\nüß† Testing analysis functionality...")
            from core.llm.models import ErrorDetails
            
            request = AnalysisRequest(
                task_type="recon",
                task_description="Test task for LLM analysis",
                success=False,
                task="test-task-123",
                execution="test-execution-456",
                error=ErrorDetails(message="Connection timeout to target host", code="CONNECTION_TIMEOUT"),
                context_data={"target_ip": "192.168.1.1", "port": 80}
            )
            
            service = get_llm_service()
            service.register_provider("test-blackbox", provider, set_as_default=True)
            
            messages = [
                LLMMessage.system(REFLEXION_SYSTEM_PROMPT),
                LLMMessage.user(build_analysis_prompt(request))
            ]
            
            analysis_response = await service.generate(messages)
            
            try:
                json_result = extract_json_from_response(analysis_response.content)
                print(f"üéØ Successfully generated analysis JSON: {json_result}")
                
                # Validate with Pydantic
                analysis = FailureAnalysis.model_validate(json_result)
                print(f"‚úÖ Analysis validation passed: {analysis.decision}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  JSON extraction failed: {e}")
                print(f"Raw response: {analysis_response.content[:200]}...")
        
        # Test service-wide functionality
        print("\nüåê Testing global LLM service...")
        service = get_llm_service()
        
        # Register additional providers
        service.register_provider("blackbox-main", provider, set_as_default=True)
        
        print(f"üìä Registered providers: {list(service.providers.keys())}")
        print(f"üîß Default provider: {service.default_provider_name}")
        stats = service.get_stats()
        print(f"üìà Service stats: {stats}")
        
        print("\n‚úÖ LLM Service tests completed successfully!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå LLM Service test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_backend_integration():
    """Test LLM service integration with the running backend"""
    print("üîó Testing backend LLM integration...")
    
    import httpx
    
    # Test if we can get the LLM status from the backend
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get("http://localhost:8000/health")
            
            if response.status_code == 200:
                health_data = response.json()
                print(f"‚úÖ Backend health: {health_data['status']}")
                
                # Try to get LLM service info (if available through API)
                try:
                    # This endpoint might not exist, but we'll try
                    llm_response = await client.post(
                        "http://localhost:8000/api/v1/missions/test/analyze",
                        json={
                            "task_type": "recon",
                            "task_description": "Test LLM functionality",
                            "success": False,
                            "error": "Connection timeout"
                        }
                    )
                    print(f"üß† LLM analysis endpoint response: {llm_response.status_code}")
                except httpx.RequestError as e:
                    print(f"‚ÑπÔ∏è  LLM analysis endpoint not available: {e}")
            else:
                print(f"‚ùå Backend health check failed: {response.status_code}")
                return False
                
    except httpx.RequestError as e:
        print(f"‚ùå Cannot connect to backend: {e}")
        return False
    
    return True

if __name__ == "__main__":
    print("üöÄ Starting LLM Service test for RAGLOX v3.0")
    print("=" * 50)
    
    try:
        # Test direct LLM service
        llm_success = asyncio.run(test_llm_service())
        
        # Test backend integration
        backend_success = asyncio.run(test_backend_integration())
        
        if llm_success and backend_success:
            print("\nüèÜ All LLM tests passed!")
            sys.exit(0)
        else:
            print(f"\nüí• Some tests failed: LLM={llm_success}, Backend={backend_success}")
            sys.exit(1)
    except KeyboardInterrupt:
        print("\n‚ö° Test interrupted")
        sys.exit(1)
    except Exception as e:
        print(f"\nüî• Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)